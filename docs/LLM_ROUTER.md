# LLM Router for Amazon Bedrock

## üéØ Overview

The **LLM Router** is an intelligent model selection layer that automatically chooses the best Amazon Bedrock model for each task based on:

- **Task complexity** (simple extraction vs. complex reasoning)
- **Cost constraints** (budget-conscious vs. premium)
- **Latency requirements** (fast response vs. quality)
- **Context size** (short vs. long documents)
- **Provider preferences** (Anthropic, Amazon, Meta, Mistral)

## üöÄ Key Features

### 1. **10 Bedrock Models Supported**

| Model | Provider | Capability | Context | Cost/1K In | Cost/1K Out | Speed | Reasoning |
|-------|----------|------------|---------|------------|-------------|-------|-----------|
| Claude 3 Haiku | Anthropic | Intermediate | 200K | $0.00025 | $0.00125 | 10/10 | 7/10 |
| Claude 3 Sonnet | Anthropic | Advanced | 200K | $0.003 | $0.015 | 7/10 | 9/10 |
| Claude 3.5 Sonnet | Anthropic | Expert | 200K | $0.003 | $0.015 | 7/10 | 10/10 |
| Claude 3 Opus | Anthropic | Expert | 200K | $0.015 | $0.075 | 4/10 | 10/10 |
| Titan Text Express | Amazon | Intermediate | 8K | $0.0008 | $0.0016 | 9/10 | 6/10 |
| Titan Text Lite | Amazon | Basic | 4K | $0.0003 | $0.0004 | 10/10 | 5/10 |
| Llama 3 8B | Meta | Intermediate | 8K | $0.0003 | $0.0006 | 9/10 | 7/10 |
| Llama 3 70B | Meta | Advanced | 8K | $0.00265 | $0.0035 | 6/10 | 8/10 |
| Mistral 7B | Mistral AI | Intermediate | 32K | $0.00015 | $0.0002 | 9/10 | 7/10 |
| Mistral Large | Mistral AI | Advanced | 32K | $0.004 | $0.012 | 7/10 | 9/10 |

### 2. **Task-Based Routing**

```python
from llm_router import LLMRouter, RoutingCriteria, TaskType

router = LLMRouter(region_name="us-east-1")

# Simple data extraction ‚Üí Fast, cheap model (Haiku or Titan Lite)
criteria = RoutingCriteria(
    task_type=TaskType.DATA_EXTRACTION,
    max_cost_per_request=0.001
)

# Complex reasoning ‚Üí Advanced model (Claude 3.5 Sonnet)
criteria = RoutingCriteria(
    task_type=TaskType.COMPLEX_REASONING,
    min_capability=ModelCapability.ADVANCED
)

# Risk assessment ‚Üí Expert model (Claude 3 Opus)
criteria = RoutingCriteria(
    task_type=TaskType.RISK_ASSESSMENT,
    min_capability=ModelCapability.EXPERT
)
```

### 3. **Smart Scoring Algorithm**

The router scores each model based on:

```python
score = (
    capability_score (20-80 points) +
    cost_efficiency (0-20 points) +
    speed_score (0-20 points) +
    reasoning_score (0-30 points for complex tasks) +
    context_window_bonus (0-20 points for long context)
)
```

## üìä Task Types

| Task Type | Description | Best Models |
|-----------|-------------|-------------|
| `SIMPLE_ANALYSIS` | Quick data parsing, simple questions | Haiku, Titan Lite, Llama 8B |
| `COMPLEX_REASONING` | Multi-step analysis, pattern detection | Claude 3.5 Sonnet, Opus |
| `DATA_EXTRACTION` | Structured data extraction | Haiku, Mistral 7B, Llama 8B |
| `SIGNAL_GENERATION` | Generate trading signals | Sonnet, Llama 70B |
| `PATTERN_RECOGNITION` | Identify market patterns | Claude 3.5 Sonnet, Mistral Large |
| `RISK_ASSESSMENT` | Evaluate risks | Claude 3 Opus, Claude 3.5 Sonnet |
| `LONG_CONTEXT` | Large documents (>10K tokens) | Claude 3.5 Sonnet, Mistral Large |
| `FAST_RESPONSE` | Speed critical | Haiku, Titan Lite, Llama 8B |
| `COST_OPTIMIZED` | Budget-conscious | Titan Lite, Mistral 7B, Llama 8B |

## üéØ Usage Examples

### Basic Usage

```python
from llm_router import LLMRouter, RoutingCriteria, TaskType

# Initialize router
router = LLMRouter(region_name="us-east-1")

# Define routing criteria
criteria = RoutingCriteria(
    task_type=TaskType.PATTERN_RECOGNITION,
    estimated_input_tokens=2000,
    estimated_output_tokens=500,
    min_capability=ModelCapability.ADVANCED
)

# Invoke with automatic model selection
response = router.invoke_model(
    prompt="Analyze this market pattern...",
    criteria=criteria,
    temperature=0.7,
    max_tokens=500
)

print(f"Selected: {response['model_name']}")
print(f"Response: {response['text']}")
```

### Cost-Optimized Bulk Processing

```python
# Process 1000 simple sentiment analyses
criteria = RoutingCriteria(
    task_type=TaskType.SIMPLE_ANALYSIS,
    max_cost_per_request=0.0001,  # Max $0.0001 each
    estimated_input_tokens=50,
    estimated_output_tokens=20
)

# Will select: Titan Lite or Llama 8B
for text in texts:
    response = router.invoke_model(
        prompt=f"Sentiment: {text}",
        criteria=criteria,
        temperature=0.1
    )
    # Cost: ~$0.000015 per request with Titan Lite
```

### Complex Reasoning

```python
# Multi-step market analysis
criteria = RoutingCriteria(
    task_type=TaskType.COMPLEX_REASONING,
    estimated_input_tokens=5000,
    estimated_output_tokens=2000,
    min_capability=ModelCapability.EXPERT,
    preferred_provider="Anthropic"
)

# Will select: Claude 3.5 Sonnet or Claude 3 Opus
response = router.invoke_model(
    prompt="Analyze these 8 data sources and identify patterns...",
    criteria=criteria,
    system_prompt="You are an expert market analyst..."
)
```

### Long Context Analysis

```python
# Analyze 50K token document
criteria = RoutingCriteria(
    task_type=TaskType.LONG_CONTEXT,
    estimated_input_tokens=50000,
    estimated_output_tokens=2000,
    min_capability=ModelCapability.ADVANCED
)

# Will select: Claude 3.5 Sonnet or Mistral Large (large context windows)
model = router.select_model(criteria)
print(f"Context window: {model.context_window:,} tokens")
```

## üìà Usage Tracking

```python
# Track usage across multiple invocations
router = LLMRouter()

# ... make multiple invoke_model() calls ...

# Get usage report
report = router.get_llm_usage_report()

print(f"Total invocations: {report['total_invocations']}")
print(f"Total cost: ${report['total_cost']:.4f}")

for model_id, stats in report['usage_by_model'].items():
    print(f"\n{stats['model_name']}:")
    print(f"  Calls: {stats['invocations']}")
    print(f"  Cost: ${stats['total_cost']:.4f}")
```

## üîß Integration with Market Hunter Agent

```python
from market_hunter_with_router import MarketHunterAgentWithRouter

# Enable dynamic LLM routing
agent = MarketHunterAgentWithRouter(
    bedrock_agent_id="YOUR_AGENT_ID",
    bedrock_agent_alias_id="YOUR_ALIAS_ID",
    enable_llm_routing=True  # ‚Üê Enable router
)

# Agent will automatically select optimal models:
# - Haiku for simple whale transaction extraction
# - Sonnet for complex pattern recognition
# - Opus for critical risk assessments

result = agent.execute_cycle(market_data)

# Get LLM usage report
llm_report = agent.get_llm_usage_report()
print(f"LLM cost this cycle: ${llm_report['total_cost']:.4f}")
```

## üí∞ Cost Comparison

### Example: 100 Market Analysis Cycles

**Without Router** (always using Claude 3 Sonnet):
- All queries: 100 cycles √ó 8 sources √ó $0.01 = **$80**

**With Router** (intelligent selection):
- Simple extraction (5 sources): 100 √ó 5 √ó $0.0003 = **$1.50**
- Complex analysis (2 sources): 100 √ó 2 √ó $0.01 = **$2.00**
- Pattern recognition (1 source): 100 √ó 1 √ó $0.015 = **$1.50**
- **Total: $5.00** ‚úÖ **94% cost reduction!**

## üé® Model Selection Flow

```
Input: Task + Criteria
       ‚Üì
   Get Candidate Models (by task type)
       ‚Üì
   Filter by Region
       ‚Üì
   Filter by Min Capability
       ‚Üì
   Filter by Provider Preference (optional)
       ‚Üì
   Score Each Model:
   - Capability score
   - Cost efficiency
   - Speed score
   - Reasoning score
   - Context bonus
       ‚Üì
   Select Highest Scoring Model
       ‚Üì
   Invoke Model
       ‚Üì
   Track Usage & Cost
```

## üöÄ Advanced Features

### Provider Preferences

```python
# Prefer Anthropic models
criteria = RoutingCriteria(
    task_type=TaskType.COMPLEX_REASONING,
    preferred_provider="Anthropic"
)

# Prefer Meta models
criteria = RoutingCriteria(
    task_type=TaskType.DATA_EXTRACTION,
    preferred_provider="Meta"
)
```

### Latency Constraints

```python
# Need response in <1 second
criteria = RoutingCriteria(
    task_type=TaskType.SIMPLE_ANALYSIS,
    max_latency_ms=1000  # Will select fastest models
)
```

### Budget Constraints

```python
# Maximum $0.001 per request
criteria = RoutingCriteria(
    task_type=TaskType.PATTERN_RECOGNITION,
    max_cost_per_request=0.001  # Will exclude expensive models
)
```

## üìä Comparison Matrix

### Speed vs Cost vs Quality

```
                   Speed    Cost      Quality
Claude 3 Haiku     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Claude 3 Sonnet    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Claude 3.5 Sonnet  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Claude 3 Opus      ‚ñà‚ñà‚ñà‚ñà     ‚ñà‚ñà        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Titan Lite         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà
Titan Express      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Llama 8B           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Llama 70B          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Mistral 7B         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Mistral Large      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
```

### Best Use Cases

| Scenario | Best Model | Why |
|----------|------------|-----|
| Extract whale transactions | Haiku / Titan Lite | Fast, cheap, structured data |
| Analyze social sentiment | Llama 8B / Mistral 7B | Cost-effective, good for classification |
| Detect complex patterns | Claude 3.5 Sonnet | Superior reasoning, pattern recognition |
| Assess trading risks | Claude 3 Opus | Highest quality, critical decisions |
| Long document analysis | Claude 3.5 Sonnet | 200K context window |
| Bulk processing (1000s) | Titan Lite | Cheapest at scale |

## üî• Performance Tips

1. **Use task-specific criteria**: Be specific about your task type for better selection
2. **Set cost constraints**: Prevent expensive models for simple tasks
3. **Enable caching**: Reuse router instance across multiple calls
4. **Track usage**: Monitor costs with `get_usage_report()`
5. **Batch when possible**: Group similar tasks to use same model

## üõ†Ô∏è Files

- **`llm_router.py`** - Main router implementation
- **`llm_router_examples.py`** - Usage examples
- **`market_hunter_with_router.py`** - Integrated Market Hunter Agent
- **`LLM_ROUTER.md`** - This documentation

## üìù Next Steps

1. Run examples: `python src/llm_router_examples.py`
2. Test with Market Hunter: `python src/example_usage.py`
3. Monitor costs: Check usage reports
4. Optimize criteria: Tune for your use case

---

**Cost savings: 80-95%** üí∞ | **Automatic model selection** ü§ñ | **10+ Bedrock models** üöÄ
